{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "import folium \n",
    "from sklearn.metrics.pairwise import (cosine_similarity, euclidean_distances, manhattan_distances) \n",
    "from IPython.display import display\n",
    "from surprise import SVD, Dataset, NormalPredictor, Reader, accuracy, KNNBasic, evaluate, BaselineOnly\n",
    "from surprise import get_dataset_dir\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV, KFold\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_stats(business_city, business_review_count, user_review_count):\n",
    "    # Read in datasets\n",
    "    df_business = pd.read_csv('../yelp_academic_dataset/yelp_business.csv')\n",
    "    df_user = pd.read_csv('../yelp_academic_dataset/yelp_user.csv')\n",
    "    df_reviews = pd.read_csv('../yelp_academic_dataset/yelp_review.csv')\n",
    "\n",
    "    \n",
    "    # Mask business dataset by 'City' & 'Review Count' \n",
    "    df_business = df_business[(df_business['city'] == business_city) & (df_business['review_count'] > business_review_count)]\n",
    "    \n",
    "    # Mask user dataset by user review count\n",
    "    df_user = df_user[df_user['review_count'] > user_review_count]\n",
    "    \n",
    "    # Truncate datasets\n",
    "    df_business_trunc = df_business[['business_id', 'name', 'city', 'state', 'review_count']]\n",
    "    df_user_trunc = df_user[['user_id', 'review_count']] \n",
    "    df_reviews_trunc = df_reviews[['review_id', 'user_id', 'business_id','stars']]\n",
    "    \n",
    "    # Merge into single dataset & Drop extra columnns\n",
    "    df_merge = df_reviews_trunc.merge(df_user_trunc, how='inner', on='user_id').merge(df_business_trunc, how='inner', on='business_id')\n",
    "    df_merge.drop(['review_id','review_count_x', 'name', 'city', 'state', 'review_count_y'], axis=1, inplace=True)\n",
    "    \n",
    "    # Limit new dataframe to Users with above 20 reviews included and businesses with above 10 reviews\n",
    "    # In tests this yeiled the most consistent results\n",
    "    df_merge = df_merge.groupby('user_id').filter(lambda group: len(group) > 20)\n",
    "    df_merge = df_merge.groupby('business_id').filter(lambda group: len(group) > 10)\n",
    "    \n",
    "    # Print Merged Dataframe Counts \n",
    "    print(\"City: %s \" %business_city)\n",
    "    print(\"Number of reviews included: %s \" %df_merge.shape[0])\n",
    "    print(\"\")\n",
    "    print(\"Number of unique users included: %s\" %df_merge['user_id'].unique().size)\n",
    "    print(\"\")\n",
    "    print(\"Number of unique businesses included: %s\" %df_merge['business_id'].unique().size)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Print Statistics of Ratings\n",
    "    print(df_merge['stars'].describe())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    # Create Reader w/ Scale to Import DataFrame into Surprise\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "    # The columns must correspond to user id, item id and ratings (in that order).\n",
    "    data = Dataset.load_from_df(df_merge[['user_id', 'business_id', 'stars']], reader)\n",
    "\n",
    "    # Establish Baseline Scoring for Model \n",
    "    # Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\n",
    "    \n",
    "    random_algo = NormalPredictor()\n",
    "\n",
    "    # Use Train_Test_Split on data\n",
    "    trainset, testset = train_test_split(data=data, test_size=.3, random_state=9)\n",
    "\n",
    "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
    "    random_algo.fit(trainset)\n",
    "    random_predictions = random_algo.test(testset)\n",
    "\n",
    "    # Then compute RMSE & MAE\n",
    "    print(\"Baseline Scores using Random Ratings\")\n",
    "    print(accuracy.rmse(random_predictions))\n",
    "    print(accuracy.mae(random_predictions))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Define Algorithm for Model\n",
    "    \n",
    "    #algo = sim_options = {'name': 'cosine',\n",
    "    #            'user_based': True  # compute  similarities between items\n",
    "    #           }\n",
    "    #algo = KNNBasic(sim_options=sim_options)\n",
    "    \n",
    "    algo = SVD()\n",
    "\n",
    "    # Use Train_Test_Split on data\n",
    "    trainset, testset = train_test_split(data=data, test_size=.3, random_state=9)\n",
    "\n",
    "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    \n",
    "    print(\"Accuracy Score Using Model\")\n",
    "    print(accuracy.rmse(predictions))\n",
    "    print(accuracy.mae(predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Toronto \n",
      "Number of reviews included: 3224 \n",
      "\n",
      "Number of unique users included: 119\n",
      "\n",
      "Number of unique businesses included: 105\n",
      "\n",
      "\n",
      "count    3224.000000\n",
      "mean        3.754342\n",
      "std         0.893219\n",
      "min         1.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         4.000000\n",
      "max         5.000000\n",
      "Name: stars, dtype: float64\n",
      "\n",
      "\n",
      "Baseline Scores using Random Ratings\n",
      "RMSE: 1.2472\n",
      "1.2471766155244717\n",
      "MAE:  0.9965\n",
      "0.9964798296857523\n",
      "\n",
      "\n",
      "Accuracy Score Using Model\n",
      "RMSE: 0.8426\n",
      "0.8425689305418099\n",
      "MAE:  0.6545\n",
      "0.6545289174222104\n",
      "CPU times: user 55.9 s, sys: 11.9 s, total: 1min 7s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_stats(\"Toronto\", 300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in datasets\n",
    "df_business = pd.read_csv('../yelp_academic_dataset/yelp_business.csv')\n",
    "df_user = pd.read_csv('../yelp_academic_dataset/yelp_user.csv')\n",
    "df_reviews = pd.read_csv('../yelp_academic_dataset/yelp_review.csv')\n",
    "\n",
    "    \n",
    "# Mask business dataset by 'City' & 'Review Count' \n",
    "df_business = df_business[(df_business['city'] == 'Toronto') & (df_business['review_count'] > 300)]\n",
    "    \n",
    "# Mask user dataset by user review count\n",
    "df_user = df_user[df_user['review_count'] > 100]\n",
    "    \n",
    "# Truncate datasets\n",
    "df_business_trunc = df_business[['business_id', 'name', 'city', 'state', 'review_count']]\n",
    "df_user_trunc = df_user[['user_id', 'review_count']] \n",
    "df_reviews_trunc = df_reviews[['review_id', 'user_id', 'business_id','stars']]\n",
    "    \n",
    "# Merge into single dataset & Drop extra columnns\n",
    "df_merge = df_reviews_trunc.merge(df_user_trunc, how='inner', on='user_id').merge(df_business_trunc, how='inner', on='business_id')\n",
    "df_merge.drop(['review_id','review_count_x', 'name', 'city', 'state', 'review_count_y'], axis=1, inplace=True)\n",
    "    \n",
    "# Limit new dataframe to Users with above 20 reviews included and businesses with above 10 reviews\n",
    "# In tests this yeiled the most consistent results\n",
    "df_merge = df_merge.groupby('user_id').filter(lambda group: len(group) > 20)\n",
    "df_merge = df_merge.groupby('business_id').filter(lambda group: len(group) > 10)\n",
    "\n",
    "# Create Reader w/ Scale to Import DataFrame into Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_merge[['user_id', 'business_id', 'stars']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>1st Recommendation</th>\n",
       "      <th>1st Est Rating</th>\n",
       "      <th>2nd Recommendation</th>\n",
       "      <th>2nd Est Rating</th>\n",
       "      <th>3rd Recommendation</th>\n",
       "      <th>3rd Est Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0uNxhZAFbkalQImzJ6UDUA</td>\n",
       "      <td>ZumOnWbstgsIE6bJlxw0_Q</td>\n",
       "      <td>4.293893</td>\n",
       "      <td>nT16Y6AsJDwEpUB1JICKzg</td>\n",
       "      <td>4.197741</td>\n",
       "      <td>nqTvE7ivdU23oUWdI01tOA</td>\n",
       "      <td>4.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-od707p4FHGul0gte29AoQ</td>\n",
       "      <td>SGP1jf6k7spXkgwBlhiUVw</td>\n",
       "      <td>4.483974</td>\n",
       "      <td>nT16Y6AsJDwEpUB1JICKzg</td>\n",
       "      <td>4.316417</td>\n",
       "      <td>nqTvE7ivdU23oUWdI01tOA</td>\n",
       "      <td>4.302581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VHc1yqBhsE-l3JQFXuWydQ</td>\n",
       "      <td>SGP1jf6k7spXkgwBlhiUVw</td>\n",
       "      <td>4.899382</td>\n",
       "      <td>nT16Y6AsJDwEpUB1JICKzg</td>\n",
       "      <td>4.859276</td>\n",
       "      <td>nqTvE7ivdU23oUWdI01tOA</td>\n",
       "      <td>4.565736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NzlqN1Ca9SW5z780thoiAg</td>\n",
       "      <td>nT16Y6AsJDwEpUB1JICKzg</td>\n",
       "      <td>4.588173</td>\n",
       "      <td>SGP1jf6k7spXkgwBlhiUVw</td>\n",
       "      <td>4.424797</td>\n",
       "      <td>pSMK_FtULKiU-iuh7SMKwg</td>\n",
       "      <td>4.358573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z6gseuVl0cR7tRLQa_DXuQ</td>\n",
       "      <td>SGP1jf6k7spXkgwBlhiUVw</td>\n",
       "      <td>4.188522</td>\n",
       "      <td>nT16Y6AsJDwEpUB1JICKzg</td>\n",
       "      <td>4.137975</td>\n",
       "      <td>7oEKIG7d1ttPRejppZ3WIA</td>\n",
       "      <td>4.074660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id      1st Recommendation  1st Est Rating  \\\n",
       "0  0uNxhZAFbkalQImzJ6UDUA  ZumOnWbstgsIE6bJlxw0_Q        4.293893   \n",
       "1  -od707p4FHGul0gte29AoQ  SGP1jf6k7spXkgwBlhiUVw        4.483974   \n",
       "2  VHc1yqBhsE-l3JQFXuWydQ  SGP1jf6k7spXkgwBlhiUVw        4.899382   \n",
       "3  NzlqN1Ca9SW5z780thoiAg  nT16Y6AsJDwEpUB1JICKzg        4.588173   \n",
       "4  z6gseuVl0cR7tRLQa_DXuQ  SGP1jf6k7spXkgwBlhiUVw        4.188522   \n",
       "\n",
       "       2nd Recommendation  2nd Est Rating      3rd Recommendation  \\\n",
       "0  nT16Y6AsJDwEpUB1JICKzg        4.197741  nqTvE7ivdU23oUWdI01tOA   \n",
       "1  nT16Y6AsJDwEpUB1JICKzg        4.316417  nqTvE7ivdU23oUWdI01tOA   \n",
       "2  nT16Y6AsJDwEpUB1JICKzg        4.859276  nqTvE7ivdU23oUWdI01tOA   \n",
       "3  SGP1jf6k7spXkgwBlhiUVw        4.424797  pSMK_FtULKiU-iuh7SMKwg   \n",
       "4  nT16Y6AsJDwEpUB1JICKzg        4.137975  7oEKIG7d1ttPRejppZ3WIA   \n",
       "\n",
       "   3rd Est Rating  \n",
       "0        4.076923  \n",
       "1        4.302581  \n",
       "2        4.565736  \n",
       "3        4.358573  \n",
       "4        4.074660  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the movielens dataset.\n",
    "trainset = data.build_full_trainset()\n",
    "# algo = sim_options = {'name': 'cosine',\n",
    "#                  'user_based': True  # compute  similarities between items \n",
    "#                 }\n",
    "    \n",
    "# algo = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "algo = SVD()\n",
    "               \n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "#predictions\n",
    "\n",
    "top_n = get_top_n(predictions, n=3)\n",
    "\n",
    "#create dataframe with top 3 recommendations by user with estimated scores and business_ids\n",
    "\n",
    "top_3df = pd.DataFrame(list(top_n.items()), columns=['user_id', 'recos'])\n",
    "top_3df[['Reco_1','Reco_2', 'Reco_3']] = pd.DataFrame(top_3df.recos.values.tolist(), index= top_3df.index)\n",
    "top_3df.drop(['recos'], axis=1, inplace=True)\n",
    "top_3df[['1st Recommendation', '1st Est Rating']] = top_3df['Reco_1'].apply(pd.Series)\n",
    "top_3df[['2nd Recommendation', '2nd Est Rating']] = top_3df['Reco_2'].apply(pd.Series)\n",
    "top_3df[['3rd Recommendation', '3rd Est Rating']] = top_3df['Reco_3'].apply(pd.Series)\n",
    "top_3df.drop(['Reco_1', 'Reco_2', 'Reco_3'], axis=1, inplace=True)\n",
    "top_3df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
